\documentclass{article}

\usepackage{neurips_2024} % acceptable for class; formatting not strictly enforced
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}

\title{Efficient Finite State Entropy: A Pedagogical Implementation and Performance Analysis}

\author{
  Aayush Gupta \\
  Department of Electrical Engineering \\
  Stanford University \\
  \texttt{aayushg55@stanford.edu}
}

\begin{document}
\maketitle

\begin{abstract}
Finite State Entropy (FSE) is a table-based realization of Asymmetric Numeral Systems (ANS) that achieves compression ratios comparable to arithmetic coding while retaining speeds closer to Huffman coding. This project presents a two-stage implementation of FSE: first, a clear and pedagogical Python reference integrated into the Stanford Compression Library (SCL), and second, an optimized C++ port designed to explore the performance gap between a readable implementation and production-grade entropy coders. Both implementations maintain bitstream compatibility and are evaluated using standard benchmarking frameworks, including \texttt{fullbench} and \texttt{lzbench}. Experimental results show that the Python implementation matches other ANS variants in compression efficiency while outperforming pure-Python rANS and tANS in throughput. The C++ implementation achieves approximately a 20$\times$ speedup over Python, with an additional 2--5$\times$ improvement from optimized bit I/O, though it remains slower than Yann Collet’s highly optimized FSE. This work provides both an accessible reference for understanding FSE and a concrete case study in bridging theoretical compression algorithms and high-performance systems.
\end{abstract}

\section{Introduction}

Modern lossless compression systems typically combine an LZ-style front end with an entropy coder that converts symbol streams into compact bit representations near the Shannon limit. Classical entropy coding techniques exhibit a long-standing trade-off: Huffman coding is simple and fast but restricted to integer-length codewords, while arithmetic and range coding achieve near-optimal compression ratios at the cost of greater computational complexity.

Asymmetric Numeral Systems (ANS), introduced by Duda~\cite{duda}, reformulate entropy coding by maintaining a single integer state that jointly represents both previously encoded bits and the next symbol to be processed. Finite State Entropy (FSE), popularized by Collet and used in Zstandard, is a table-based realization of ANS that pushes most arithmetic into precomputed tables, enabling extremely fast encoding and decoding while retaining compression efficiency comparable to arithmetic coding.

The primary goal of this project is pedagogical and exploratory rather than purely competitive. We aim to (1) reimplement FSE in pure Python in a way that emphasizes clarity and correctness, (2) port this implementation to C++ while preserving bitstream compatibility, and (3) quantitatively evaluate how successive low-level optimizations narrow the performance gap to production-grade implementations. In doing so, we seek to illuminate both the algorithmic structure of FSE and the systems-level considerations that dominate real-world performance.

\section{Literature Review}

\subsection{Asymmetric Numeral Systems}

Duda’s ANS framework~\cite{duda} demonstrates that arithmetic-coding-level compression efficiency can be achieved using a single integer state rather than an interval. Symbols are mapped to subsets of a finite state space in proportion to their probabilities, and encoding and decoding proceed by updating this state while emitting or consuming bits as needed.

Two major practical variants exist. Range ANS (rANS) resembles traditional range coding and relies on integer multiplications and divisions in its inner loop. Table-based ANS (tANS), in contrast, precomputes the necessary arithmetic into lookup tables, replacing expensive operations with memory accesses and simple bit manipulations.

\subsection{Finite State Entropy}

Finite State Entropy is a carefully engineered instance of tANS optimized for modern CPUs~\cite{collet-blog}. FSE begins by normalizing symbol frequencies so that their sum equals a power of two, $2^{\texttt{tableLog}}$, defining the size of the state space. Symbols are then spread across this state space using a deterministic stepping scheme that approximates their desired probabilities.

From this construction, a decode table is built in which each state stores a symbol, the number of bits to read, and a base for computing the next state. Encoding proceeds in reverse by scanning symbols backward and using per-symbol transforms to reproduce the same state transitions. Crucially, while FSE allows flexibility in how frequencies are normalized and states are assigned, correctness depends on strict consistency between encoder and decoder tables; suboptimal choices primarily degrade compression efficiency rather than validity.

\subsection{Related Implementations}

Collet’s reference FSE implementation~\cite{fse-github} serves as the performance baseline for this project. Zstandard~\cite{zstd} combines FSE and Huffman coding with an LZ77 front end, achieving state-of-the-art performance. Within the Stanford Compression Library (SCL)~\cite{scl}, several entropy coders—including Huffman, arithmetic coding, rANS, and tANS—are implemented in Python, providing a natural environment for a pedagogical FSE reference and comparative evaluation.

\section{Methods}

\subsection{Python Reference Implementation}

The Python implementation of FSE is integrated into SCL using its existing \texttt{DataEncoder} and \texttt{DataDecoder} abstractions. The implementation follows the standard FSE pipeline: frequency normalization, symbol spreading, decode table construction, and encoder table generation. Emphasis is placed on readability and explicitness, with each step implemented in a direct and traceable manner.

The resulting bitstream format stores the block size, final state, and payload bits in a manner compatible with both Python and C++ implementations. Although Python allows considerable flexibility in normalization and state assignment, the implementation adheres to conventional choices to produce competitive compression ratios while remaining easy to reason about.

\subsection{C++ Implementation and Optimization}

The C++ implementation mirrors the Python design to ensure bitstream compatibility, enabling cross-language validation. The core functionality is implemented as a static library, with optimized MSB and LSB bit readers and writers. To facilitate testing and benchmarking, the C++ codec is exposed to Python via pybind11, allowing existing SCL tests to be reused without modification.

Performance optimizations are introduced incrementally, including improved bit I/O and multi-block framing. This staged approach allows us to isolate the contribution of each optimization to overall throughput.

\subsection{Testing and Validation}

Given the subtle consistency requirements between encoder and decoder tables, testing focuses on verifying round-trip correctness rather than enforcing a single “exact” construction. Unit tests validate normalization, spreading, and table construction, while integration tests ensure that Python-encoded streams can be decoded by the C++ implementation and vice versa. This cross-language validation provides strong assurance of correctness while accommodating the inherent flexibility of FSE’s design space.

\subsection{Benchmarking Setup}

Evaluation uses both custom Python benchmarks and established external harnesses. Entropy-only performance is measured using \texttt{fullbench}, while end-to-end performance is evaluated using \texttt{lzbench}. Experiments are conducted on the Canterbury and Silesia corpora as well as synthetic distributions, reporting compression ratio and encode/decode throughput.

\section{Results and Analysis}

\subsection{Python Performance}

The Python FSE implementation achieves compression ratios comparable to other ANS variants and consistently outperforms pure-Python rANS and tANS in throughput, while remaining slower than Huffman coding. These results align with theoretical expectations: FSE avoids the arithmetic overhead of rANS but incurs more complexity than Huffman’s codeword lookup.

\subsection{C++ Performance}

Porting the implementation to C++ yields approximately a 20$\times$ speedup, with optimized bit I/O contributing an additional 2--5$\times$ improvement. Figure~\ref{fig:fullbench} compares entropy-only throughput against Collet’s reference FSE. While compression ratios are similar, a performance gap of roughly 2.5$\times$ remains, attributable to factors such as table rebuild overhead, conservative bit I/O, and lack of interleaved multi-state decoding.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{full_bench_plot.pdf}
  \caption{Entropy-only throughput comparison using \texttt{fullbench}.}
  \label{fig:fullbench}
\end{figure}

End-to-end benchmarks using \texttt{lzbench} (Figure~\ref{fig:lzbench}) show that this FSE implementation trails production codecs such as zstd and lz4, which benefit from sophisticated LZ front ends in addition to highly optimized entropy stages.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\linewidth]{encode.pdf}
  \includegraphics[width=0.48\linewidth]{decode.pdf}
  \caption{Encode and decode throughput versus compression ratio on the Silesia corpus using \texttt{lzbench}.}
  \label{fig:lzbench}
\end{figure}

\section{Conclusions}

This project presents a complete, readable implementation of Finite State Entropy in both Python and C++, illustrating how ANS-based entropy coding transitions from theory to practice. The Python version serves as a pedagogical reference, while the C++ port demonstrates the substantial performance gains achievable through low-level optimization. Although a gap remains relative to production-grade FSE, the results clarify where engineering effort yields the greatest returns.

\section*{Limitations and Future Work}

Several limitations suggest directions for future work. Header overhead could be reduced through normalized counter compression. Interleaved multi-state decoding would improve instruction-level parallelism, and further bit I/O optimizations could reduce hot-path overhead. Adaptive table sizing and improved handling of low-frequency symbols may also yield better compression–speed trade-offs.

\section*{Acknowledgments}

We thank Pulkit and Shubham for their mentorship and the EE274 teaching staff for guidance throughout this project.

\bibliographystyle{plain}
\begin{thebibliography}{5}
\bibitem{duda} J. Duda. Asymmetric numeral systems. arXiv:1311.2540, 2013.
\bibitem{collet-blog} Y. Collet. Finite State Entropy. Fast Compression Blog.
\bibitem{fse-github} Y. Collet. FiniteStateEntropy library. GitHub.
\bibitem{zstd} Y. Collet et al. Zstandard. GitHub.
\bibitem{scl} Stanford Compression Library. GitHub.
\end{thebibliography}

\end{document}